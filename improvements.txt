Submissions:
- submission1 (failed)
- submission 2
    = svm with 900 features (30 x 30 resizing), first 100 of each class (9045 training)
- submission 3
    = same as submission 2 but with full training
- submission 4
    = added additional feature of segmented width-to-height ratio
    = else, same as submission 3
- submission 5
    = width-height ratio from before
    = used computeFeatures.shape, computeFeatures.shape on resized image (13 features)
    = SVM
- submission 6
    = same as above, with computeFeatures.moment, computeFeatures.haralick, and 30 x 30 pixels
    = RBF (radial basis) kernel instead of linear
- submission 7
    = same as above
    = add high pass version of all features (except 30x30)
    = add low pass version of all features (except 30x30)
    = with these additional features (total should be around 1100), use semi-supervised learning
        * run clustering algorithm on the unlabeled testing set
            - mclust
        * get probabilities from that clustering algorithm
            - note, must figure out mapping from classes to clusters
        * somehow average the probabilities produced by the supervised SVM on testing set, and the probabilities produced by the clustering on the testing set

- submission 8
    = next ideas:
        * low-pass (removes noise, blurs image)
            - filter2, gblur
        * high-pass (keeps the edges)
            - filter2
        * medianFilter
        * weighted SVM with boosting
            - http://cran.r-project.org/web/packages/wSVM/wSVM.pdf
        * use Conv Neural Net
            - http://deeplearning.net/tutorial/lenet.html
        * use semi-supervised learning
